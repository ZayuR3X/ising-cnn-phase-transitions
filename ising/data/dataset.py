"""
ising/data/dataset.py

PyTorch Dataset and DataLoader factories for the Ising Model spin configurations.

Reads from the HDF5 files generated by generator.py and provides:
  - On-the-fly symmetry augmentation (rotations, reflections, spin flip)
  - Stratified train/val/test splits by temperature
  - Critical region oversampling
"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from pathlib import Path
from typing import Optional, List, Tuple
import h5py


class IsingDataset(Dataset):
    """
    PyTorch Dataset for 2D Ising spin configurations stored in HDF5.

    Parameters
    ----------
    h5_path    : path to HDF5 file produced by generator.py
    indices    : optional subset of global indices (for train/val/test split)
    augment    : apply symmetry augmentations (rotation + reflection)
    spin_flip  : also apply global spin flip augmentation
    """

    def __init__(
        self,
        h5_path: str,
        indices: Optional[np.ndarray] = None,
        augment: bool = False,
        spin_flip: bool = False,
    ):
        self.h5_path   = str(h5_path)
        self.augment   = augment
        self.spin_flip = spin_flip

        # Load metadata (keep file closed except during __getitem__)
        with h5py.File(self.h5_path, "r") as hf:
            n_total = hf["spins"].shape[0]
            self.J  = float(hf.attrs.get("J", 1.0))
            self.L  = int(hf.attrs.get("L", 64))

        self.indices = (
            np.arange(n_total) if indices is None
            else np.asarray(indices)
        )

        # Augmentation multiplier for __len__
        self._aug_factor = 1
        if augment:
            self._aug_factor *= 4   # 0°, 90°, 180°, 270°
        if spin_flip:
            self._aug_factor *= 2   # original + flipped

    def __len__(self) -> int:
        return len(self.indices) * self._aug_factor

    def __getitem__(self, idx: int) -> dict:
        base_idx  = idx // self._aug_factor
        aug_code  = idx  % self._aug_factor
        raw_idx   = int(self.indices[base_idx])

        with h5py.File(self.h5_path, "r") as hf:
            cnn_input = hf["cnn_input"][raw_idx].copy()   # (4, L, L) float32
            phase     = int(hf["phase"][raw_idx])
            T         = float(hf["temperature"][raw_idx])
            m         = float(hf["magnetization"][raw_idx])
            E         = float(hf["energy"][raw_idx])

        # --- Symmetry augmentation ---
        if self.augment or self.spin_flip:
            cnn_input = self._augment(cnn_input, aug_code)

        # Physics features vector: [m, chi_approx, E, beta]
        # chi approximation from single config: m^2 * L^2 / T (rough)
        chi_approx = (m ** 2) * (self.L ** 2) / T
        physics    = np.array([m, chi_approx, E, 1.0 / T], dtype=np.float32)

        return {
            "cnn_input":    torch.from_numpy(cnn_input),
            "physics":      torch.from_numpy(physics),
            "phase":        torch.tensor(phase, dtype=torch.long),
            "T":            torch.tensor(T, dtype=torch.float32),
            "m":            torch.tensor(m, dtype=torch.float32),
        }

    def _augment(self, x: np.ndarray, code: int) -> np.ndarray:
        """Apply deterministic augmentation based on aug_code."""
        rot  = code % 4 if self.augment else 0
        flip = (code // 4) if self.spin_flip else 0

        # Rotation (last two spatial dims)
        if rot > 0:
            x = np.rot90(x, k=rot, axes=(1, 2)).copy()

        # Global spin flip: only channels 0 (raw spins) and 2 (coarse mag)
        # channels 1 (energy) and 3 (Fourier) are spin-flip invariant
        if flip:
            x = x.copy()
            x[0] = -x[0]   # raw spins
            x[2] = -x[2]   # coarse magnetization

        return x


# ---------------------------------------------------------------------------
# Train/val/test split
# ---------------------------------------------------------------------------

def stratified_split(
    h5_path: str,
    train_frac: float = 0.70,
    val_frac:   float = 0.15,
    critical_strip: Optional[Tuple[float, float]] = (2.15, 2.40),
    seed: int = 42,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Split dataset indices into train/val/test sets, stratified by temperature.

    Configs in the critical strip [T_lo, T_hi] are preferentially placed in
    the validation set as the hardest generalization test.

    Returns
    -------
    train_idx, val_idx, test_idx : numpy index arrays
    """
    rng = np.random.default_rng(seed)

    with h5py.File(h5_path, "r") as hf:
        T_all = hf["temperature"][:].astype(np.float32)
        n     = len(T_all)

    unique_T = np.unique(np.round(T_all, 4))
    train_idx, val_idx, test_idx = [], [], []

    for T in unique_T:
        mask = np.where(np.abs(T_all - T) < 1e-3)[0]
        rng.shuffle(mask)

        # Critical strip → goes to val
        if critical_strip is not None:
            T_lo, T_hi = critical_strip
            if T_lo <= T <= T_hi:
                val_idx.extend(mask.tolist())
                continue

        n_T    = len(mask)
        n_train = max(1, int(np.floor(train_frac * n_T)))
        n_val   = max(1, int(np.floor(val_frac   * n_T)))

        train_idx.extend(mask[:n_train].tolist())
        val_idx.extend(  mask[n_train : n_train + n_val].tolist())
        test_idx.extend( mask[n_train + n_val:].tolist())

    return (
        np.array(train_idx),
        np.array(val_idx),
        np.array(test_idx),
    )


# ---------------------------------------------------------------------------
# DataLoader factory
# ---------------------------------------------------------------------------

def make_dataloaders(
    h5_path: str,
    batch_size: int = 128,
    train_frac: float = 0.70,
    val_frac:   float = 0.15,
    critical_strip: Optional[Tuple[float, float]] = (2.15, 2.40),
    critical_oversample_factor: int = 3,
    augment: bool = True,
    spin_flip: bool = True,
    num_workers: int = 2,
    seed: int = 42,
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    Build train, validation, and test DataLoaders from a single HDF5 file.

    Training set:
      - Symmetry augmentation enabled
      - Critical region oversampled by `critical_oversample_factor`

    Validation / test sets:
      - No augmentation
      - Sequential sampling

    Returns
    -------
    train_loader, val_loader, test_loader
    """
    train_idx, val_idx, test_idx = stratified_split(
        h5_path, train_frac, val_frac, critical_strip, seed
    )

    train_ds = IsingDataset(h5_path, train_idx, augment=augment, spin_flip=spin_flip)
    val_ds   = IsingDataset(h5_path, val_idx,   augment=False,   spin_flip=False)
    test_ds  = IsingDataset(h5_path, test_idx,  augment=False,   spin_flip=False)

    # --- Critical region oversampling for train ---
    with h5py.File(h5_path, "r") as hf:
        sort_order = np.argsort(train_idx)
        sorted_idx = train_idx[sort_order]
        T_train_sorted = hf["temperature"][sorted_idx].astype(np.float32)
        T_train = T_train_sorted[np.argsort(sort_order)]

    Tc = 2.2692
    weights = np.ones(len(train_idx), dtype=np.float32)
    if critical_strip is not None:
        T_lo, T_hi = critical_strip
        near_tc = (T_train >= T_lo) & (T_train <= T_hi)
        weights[near_tc] *= critical_oversample_factor

    # Tile weights for augmentation factor
    weights_tiled = np.tile(weights, train_ds._aug_factor)
    sampler = WeightedRandomSampler(
        weights=torch.from_numpy(weights_tiled),
        num_samples=len(train_ds),
        replacement=True,
    )

    train_loader = DataLoader(
        train_ds, batch_size=batch_size,
        sampler=sampler, num_workers=num_workers,
        pin_memory=True, drop_last=True,
    )
    val_loader = DataLoader(
        val_ds, batch_size=batch_size,
        shuffle=False, num_workers=num_workers,
    )
    test_loader = DataLoader(
        test_ds, batch_size=batch_size,
        shuffle=False, num_workers=num_workers,
    )

    return train_loader, val_loader, test_loader
